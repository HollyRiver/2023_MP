{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d50df5-bcd4-49d1-a13c-4fc8df4ddd8c",
   "metadata": {},
   "source": [
    "## code summation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81fcd3-580b-4406-b44b-9fa75d321662",
   "metadata": {},
   "source": [
    "### **1. Analysis and Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd608422-33f4-4a2a-89e0-df8a4d36778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## 왠만해선 그냥 sklearn을 import하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2bfdf-8df6-4154-800c-4bea66616c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('file derectort')  ## csv 데이터를 DataFrame으로 읽어온다. 직접 입력 또는 간접(.) 입력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172ea5a-0fde-42f3-84dc-622944ef2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('file name', index = bool)  ## DataFrame을 현재 디렉토리에 csv 파일로 저장한다. index = False로 지정 시 인덱스를 누락시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4743be8-382c-43a7-af09-36f53b4592a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df)  ## DataFrame의 범주형 자료를 포함한 열을 더미변수로 만들어준다. 회귀에 유리하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb20f23-083c-45a4-84c9-c1a9fb73955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier(n_estimators = int, max_depth = int, random_state = int)  ## 뭔진 모르겠지만 아무튼 추정하는 모델이다. predictr에 저장하여 predictr.fit(X, y)해주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d1996-a924-4860-aa66-ab4fb5d8fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## analysis cycle, 분석 과정\n",
    "##----1. data----\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "X = train.loc[:, features]\n",
    "y = train.loc[:, response]\n",
    "XX = test.loc[: features]\n",
    "##yy = test.loc[:, response] ## 보통 주어지지 않음\n",
    "\n",
    "##----2. predictor----\n",
    "predictr = ...model...\n",
    "\n",
    "##----3. fitting----\n",
    "predictr.fit(X, y)\n",
    "predictr.score(X, y)  ## or another method\n",
    "\n",
    "##----4. submission----\n",
    "test.assign(response = predictr.predict(XX)).loc[:, response].to_csv('file name', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824385c6-963b-4cdf-8aaf-8ecd990c9d89",
   "metadata": {},
   "source": [
    "### **2. autogluon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f8366b-eb58-409f-9624-e561c323e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install autogluon  ## colab or kaggle notebook. 이미 환경이 구성되어 있다면 필요없음.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e89b5c-de31-46ad-a5d0-5a49026fa135",
   "metadata": {},
   "outputs": [],
   "source": [
    "TabularDataset('file derectory')  ## pd.read_csv()와 비슷한 코드라고 보면 된다. 실제로 해당 개체를 다루는 방식이 거의 유사하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb2df1-1e4f-4c85-9576-beb422c5f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## analysis cycle\n",
    "\n",
    "#----1. data----\n",
    "train = TabularDataset('train.csv directory')\n",
    "test = TabularDataset('test.csv directory')\n",
    "\n",
    "#X = train.drop([반응변수 열], axis = 1)\n",
    "#y = train[반응변수 열]\n",
    "#XX = test.drop([반응변수 열], axis = 1)\n",
    "#상기 과정이 필요없다.\n",
    "\n",
    "#----2. create predictor----\n",
    "predictr = TabularPredictor(반응변수 열)\n",
    "\n",
    "#----3. fitting----\n",
    "predictr.fit(train)  ## 다른 선형회귀에선 설명변수와 반응변수를 나눠줬지만, 여기선 한번에 입력했다. 이미 predictr에 무엇이 반응변수인지 지정해줬기 때문이다.\n",
    "(train[반응변수 열] == predictr.predict(X)).mean()  ## 몇 퍼센트가 맞는 지 산출해준다. score에 해당한다.\n",
    "\n",
    "#----4. submittion----\n",
    "test.assign(반응변수 열 = predictr.predict(XX)).loc[:, 제출에 필요한 열].to_csv(디렉토리, index = bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba18c09-96db-4e1c-a9e8-97d6be659628",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictr.leaderboard()  ## fitting을 마친 predictr가 사용한 모델들의 성능 순위가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6ecdc0-382d-4788-8ab5-aaf9be111f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "# 유사한 정보를 가지고 있는 여러 열들을 하나로 합치면 더 좋은 결과가 나온다(다중 공선성 해결 등)\n",
    "train.assign(Fsize = train.Sibsp + train.Parch).drop(['Sibsp', 'Parch'], axis = 1)  ## 동승한 가족의 수를 합하고, 기존 자료는 제거해야 한다. 그렇지 않으면 다중공선성 문제가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361b10e-98b0-428d-ba38-1b02cdd7f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictr.fit(train, presets = 'best_quality')  ##  presets에 설정된 옵션 소환. 자원을 전부 지원해줄 테니, 가장 좋은 퀄리티를 내놓아라. > 시간 오래걸림, 성능 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c11e2b-8210-49b9-8ddc-1f8a938ad430",
   "metadata": {},
   "source": [
    "### **3. Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf1349c-46b2-480d-a64e-eae48f8ae1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f30629-c73f-4e53-817c-715dde09b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cycle\n",
    "\n",
    "## step 1\n",
    "X = pd.get_dummies(df_train.drop([반응변수 열], axis = 1))  ## 범주형 자료가 있을 때 반드시 사용\n",
    "y = df_train[반응변수열]\n",
    "XX = pd.get_dimmies(df_train)\n",
    "\n",
    "## step 2\n",
    "predictr = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "## step 3\n",
    "predictr.fit(X, y)\n",
    "\n",
    "## step 4\n",
    "predictr.predict(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253dab27-ec60-4aa9-b0cf-b2d69500b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictr.coef_  ## coefficient, 계수를 내준다.\n",
    "predictr.intercept_  ## 절편을 내준다. 둘다 array로 반환하므로 사용 시 변환이 필요\n",
    "predictr.score(X, y)  ## R^2_score 값을 산출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9cc0b-33cc-486d-ac14-ea2019dbaa24",
   "metadata": {},
   "source": [
    "### **4. Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8509f291-26bf-4c12-938f-bae2318f6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6275e-ba9b-4b2f-96e8-1d43fa74f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cycle\n",
    "\n",
    "## step 1\n",
    "X = pd.get_dummies(df_train.drop([반응변수 열], axis = 1))\n",
    "y = df_train[반응변수 열]\n",
    "XX = pd.get_dummies(df_test)\n",
    "\n",
    "## step 2\n",
    "predictr = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "## step 3\n",
    "predictr.fit(X, y)\n",
    "\n",
    "## step 4\n",
    "predictr.predict(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ad1ad-f4fc-4fb5-89a5-931a68a2e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictr.score(X, y)  ## 예측한 반응변수가 원 반응변수 열 대비 얼마나 맞는 지 산출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a772412-1ce1-45ee-9ed7-35f6d872a1d4",
   "metadata": {},
   "source": [
    "### **5. 결측치 처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1472de-f878-4280-a169-a342e8722235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import sklearn.impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9004e-106f-413b-ad9b-d4987a38ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info  ## 해당 결과 시 결측치가 많아보인다면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f8b08-644e-4b82-a3ba-13819cdc0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df)  ## 특정 행에 결측치가 얼마나 있는지 시각화한다.\n",
    "msno.heatmap(df)  ## 결측치가 있는 구조(특정 행 부분에 밀집된 정도)가 비슷한 것 끼리 상관계수 내듯이\n",
    "msno.dendrogram(df)  ## 결측치 존재 구조가 비슷한 행끼리 엮어놓음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb9b93-241b-406a-8792-034e15f9b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputr = sklearn.impute.SimpleImputer(strategy = '통계량 타입', fill_value = '채울 값')\n",
    "## strategy = 'mean', 'most_frequent', 'median',  'constant' > fill_value = 'value'\n",
    "imputr.fit(df)\n",
    "imputr.transform(df)  ## imputr.fit_transform(df) > 피팅과 전환을 한번에!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977da07-762c-462e-9758-0ac97fdbbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(imputr.fit_transform(df).reshape(-1))\n",
    "## impute 후 pd.Series로 전환할 때 꼭 1차원 배열로 바꿀 것! Series는 2차원 이상의 배열을 받을 수 없다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab213d5e-3c9e-4fc7-afdd-7bd628191f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputr_int = sklearn.impute.SimpleImputer(strategy = 'mean')\n",
    "imputr_obj = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "imputr_int.fit_transform(df.select_dtypes(include = 'number'))\n",
    "imputr_int.fit_transform(df.select_dtypes(exclude = 'number'))\n",
    "\n",
    "## df.select_dtypes()를 통해 형식에 따라 데이터프레임을 손쉽게 나누고, 각자 impute할 대상을 지정해줄 수 있다.\n",
    "## 일반적으로 범주형은 최빈값, 연속형은 평균으로 impute한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d1425-4202-4b38-9096-caea451f10e5",
   "metadata": {},
   "source": [
    "### **6. 로지스틱에서의 결측치 처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5627bdf8-58b6-47c9-805b-65b6a8dbdd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.impute\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46c0f9-7b29-4af9-8aec-0cebbc6f8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing(df):\n",
    "    \"\"\"\n",
    "    It impute missing, output imputed DataFrame.\n",
    "    \n",
    "    df : DataFrame include NaN value\n",
    "    \"\"\"\n",
    "    df_ = df.copy()  ## 데이터를 복사, 기존 데이터를 바꾸게 될 수도 있으므로 매우 유용하다.\n",
    "    \n",
    "    df_num = df_.select_dtypes(include = 'number')  ## 해당하는 데이터 타입만 선택\n",
    "    df_obj = df_.select_dtypes(exclude = 'number')\n",
    "    \n",
    "    df_[df_num.columns] = sklearn.impute.SimpleImputer(strategy = 'mean').fit_transform(df_num)\n",
    "    df_[df_obj.columns] = sklearn.impute.SimpleImputer(strategy = 'most_frequent').fit_transform(df_obj)\n",
    "    \n",
    "    return df_\n",
    "\n",
    "## imputing하는 함수를 만든다. train, test셋을 다루려면 최소한 두 번은 imputing을 해야 하니... 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fce7a0-c17a-457c-8723-98feab6da594",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(impute_missing(df_train))  ## 범주형 열에 사용 시 주의해야 한다. 고유값을 가지는 열(이름, 티켓번호 등...)의 경우 더미를 행의 수만큼 만들수도 있음...\n",
    "## 꼭 필요없는 열은 드롭하고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b759c-7c3e-4d80-9275-24e203804008",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 1\n",
    "X = pd.get_dummies(impute_missing(df_train.drop([고유값을 가지는 열들, 반응변수 열], axis = 1)))\n",
    "y = df_train['반응변수 열']\n",
    "XX = pd.get_dummies(impute_missing(df_test.drop([고유값을 가지는 열들], axis = 1)))\n",
    "\n",
    "## step 2\n",
    "## 기본 로지스틱 회귀분석 방법과 동일..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6bd10-fca7-42f5-a349-cbb3b583ee62",
   "metadata": {},
   "source": [
    "### **7. predictor의 이해**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ac9f2b-ddcc-4fbb-95fd-48c8a589ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53ac10-3300-4a92-819b-ff715f9057ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 로지스틱 회귀분석의 원리\n",
    "## 해당 개체에 처리가 취해질 확률이 0.5보다 크면 1, 0.5보다 작으면 0\n",
    "\n",
    "## 1\n",
    "X = df.drop(['employment'], axis = 1)\n",
    "y = df.employment\n",
    "\n",
    "## 2\n",
    "predictr = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "## 3\n",
    "predictr.fit(X, y)\n",
    "\n",
    "## 4\n",
    "predictr.predict(X)  ## yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d7228-a87e-4244-b4b6-56990d84e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 상기 과정에서 각 개체의 확률을 알고 싶으면...\n",
    "predictr.predict_proba(X)  ## n by 2의 행렬을 산출한다. 첫 행은 0일 확률, 둘째 행은 1일 확률이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c586a-3f27-4699-b7ff-aeb0d743f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = {'DataFrame(2d)': df_train_X, \n",
    "      'Seires(1d)': df_train_X.X,\n",
    "      'ndarray(2d)': np.array(df_train_X),\n",
    "      'ndarray(1d)': np.array(df_train_X).reshape(-1),\n",
    "      'list(2d)': np.array(df_train_X).tolist(),\n",
    "      'list(1d)': np.array(df_train_X).reshape(-1).tolist()}\n",
    "\n",
    "ys = {'DataFrame(2d)': df_train_y, \n",
    "      'Seires(1d)': df_train_y.y,\n",
    "      'ndarray(2d)': np.array(df_train_y),\n",
    "      'ndarray(1d)': np.array(df_train_y).reshape(-1),\n",
    "      'list(2d)': np.array(df_train_y).tolist(),\n",
    "      'list(1d)': np.array(df_train_y).reshape(-1).tolist()}\n",
    "\n",
    "def test(X,y):\n",
    "    try: \n",
    "        predictr = sklearn.linear_model.LinearRegression()\n",
    "        predictr.fit(X,y)\n",
    "        return 'no error'\n",
    "    except:\n",
    "        return 'error'  ## 예외사항(error) 발생 시의 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5989a3-3369-4ae7-8c8d-6fa61a10d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## X, y에 들어갈 수 있는 형식\n",
    "\n",
    "{('X='+i,'y='+j): test(Xs[i],ys[j]) for i,j in itertools.product(Xs.keys(),ys.keys())}\n",
    "\n",
    "## itertools.product() : 원소들의 데카르트 곱을 리스트로 반환.\n",
    "## itertools.product('ABCD', repeat = 2)의 경우 크기가 2인 앞의 string 조합을 모두 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdddd699-b680-43e7-921c-1dd8045d2f0e",
   "metadata": {},
   "source": [
    "```\n",
    "{('X=DataFrame(2d)', 'y=DataFrame(2d)'): 'no error',\r\n",
    " ('X=DataFrame(2d)', 'y=Seires(1d)'): 'no error',\r\n",
    " ('X=DataFrame(2d)', 'y=ndarray(2d)'): 'no error',\r\n",
    " ('X=DataFrame(2d)', 'y=ndarray(1d)'): 'no error',\r\n",
    " ('X=DataFrame(2d)', 'y=list(2d)'): 'no error',\r\n",
    " ('X=DataFrame(2d)', 'y=list(1d)'): 'no error',\r\n",
    " ('X=Seires(1d)', 'y=DataFrame(2d)'): 'error',\r\n",
    " ('X=Seires(1d)', 'y=Seires(1d)'): 'error',\r\n",
    " ('X=Seires(1d)', 'y=ndarray(2d)'): 'error',\r\n",
    " ('X=Seires(1d)', 'y=ndarray(1d)'): 'error',\r\n",
    " ('X=Seires(1d)', 'y=list(2d)'): 'error',\r\n",
    " ('X=Seires(1d)', 'y=list(1d)'): 'error',\r\n",
    " ('X=ndarray(2d)', 'y=DataFrame(2d)'): 'no error',\r\n",
    " ('X=ndarray(2d)', 'y=Seires(1d)'): 'no error',\r\n",
    " ('X=ndarray(2d)', 'y=ndarray(2d)'): 'no error',\r\n",
    " ('X=ndarray(2d)', 'y=ndarray(1d)'): 'no error',\r\n",
    " ('X=ndarray(2d)', 'y=list(2d)'): 'no error',\r\n",
    " ('X=ndarray(2d)', 'y=list(1d)'): 'no error',\r\n",
    " ('X=ndarray(1d)', 'y=DataFrame(2d)'): 'error',\r\n",
    " ('X=ndarray(1d)', 'y=Seires(1d)'): 'error',\r\n",
    " ('X=ndarray(1d)', 'y=ndarray(2d)'): 'error',\r\n",
    " ('X=ndarray(1d)', 'y=ndarray(1d)'): 'error',\r\n",
    " ('X=ndarray(1d)', 'y=list(2d)'): 'error',\r\n",
    " ('X=ndarray(1d)', 'y=list(1d)'): 'error',\r\n",
    " ('X=list(2d)', 'y=DataFrame(2d)'): 'no error',\r\n",
    " ('X=list(2d)', 'y=Seires(1d)'): 'no error',\r\n",
    " ('X=list(2d)', 'y=ndarray(2d)'): 'no error',\r\n",
    " ('X=list(2d)', 'y=ndarray(1d)'): 'no error',\r\n",
    " ('X=list(2d)', 'y=list(2d)'): 'no error',\r\n",
    " ('X=list(2d)', 'y=list(1d)'): 'no error',\r\n",
    " ('X=list(1d)', 'y=DataFrame(2d)'): 'error',\r\n",
    " ('X=list(1d)', 'y=Seires(1d)'): 'error',\r\n",
    " ('X=list(1d)', 'y=ndarray(2d)'): 'error',\r\n",
    " ('X=list(1d)', 'y=ndarray(1d)'): 'error',\r\n",
    " ('X=list(1d)', 'y=list(2d)'): 'error',\r\n",
    " ('X=list(1d)', 'y=list(1d)'): 'error'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea526b9-4346-42f4-b007-e8fd2941a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## X에는 2차원 데이터만, y에는 1차원 2차원 모두 올 수 있다. y는 1차원 데이터를 은근히 바라고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca2548-1cfa-4a80-8388-695420b91df5",
   "metadata": {},
   "source": [
    "### **8. 스케일링**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71123b85-e487-4a42-9d38-86c164658fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccade47-ebb8-4298-9456-184776d25df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputr = sklearn.preprocessing.MinMaxScaler()\n",
    "imputr.fit_transform(X)\n",
    "imputr.transform(XX)\n",
    "## sklearn.preprocessing.minmax_scale(X)  ## 잘 쓰진 않는다. XX에 똑같은 변환을 못해줌\n",
    "\n",
    "imputr = sklearn.preprocessing.StandardScaler()\n",
    "imputr.fit_transform(X)\n",
    "imputr.transform(XX)\n",
    "\n",
    "## X에서 fitting했으면 XX에는 fitting하지 않고 그대로 변환해주는 게 합리적이다.\n",
    "## X와 XX를 합쳐서 fitting하면 실격이다. 정보누수임(실제로는 그럴 일이 없으니까)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22bfa4-c25d-42c3-8c6c-07d33c1b66a1",
   "metadata": {},
   "source": [
    "1. **MinMaxSclaer**:\n",
    "    * 장점 : 원하는 범위 내로 데이터를 조정할 때 유용, 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력값을 조정하는 데 유용.\n",
    "    * 단점 : 이상치에 매우 민감하다.\n",
    "\n",
    "1. **StandardScaler**:\n",
    "   * 장점 : **이상치에 덜 민감**함, 많은 통계적 기법들 - **선형 알고리즘에서 잘 작동**함\n",
    "   * 단점 : 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.\n",
    "  \n",
    "> 단순히 MinMaxScaler는 데이터가 0\\~1 또는 -1~1사이의 범위에 있다고 가정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af1602-bcbe-452c-ab58-2e9616a5f7ac",
   "metadata": {},
   "source": [
    "### **9. 오버피팅, 다중공선성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748d95b3-95b3-4042-aac4-1f30600595d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677de24-f505-4c7a-a4cf-c51b46c6725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'str{}ing'.format(중괄호 안에 입력할 값)  ## 스트링 안에 중괄호가 있다면 언제든지 가능\n",
    "f'str{입력값:.4f}ing'  ## f스트링, 괄호 안에 특정 입력값을 넣어줄 수 있다.\n",
    "r'string'  ## markdown 문법으로 수식 작성 가능, ex) $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17125fd-a3b1-4290-93d7-c51bc0c1133f",
   "metadata": {},
   "source": [
    "> 반응변수와 관련성이 낮은 설명변수는(없는 설명변수) 오버피팅을 유발한다. 즉, 오차항을 적합하게 만든다.\n",
    ">\n",
    "> 설명변수끼리의 상관성이 높은 경우, 해당 설명변수들이 반응변수와의 상관성이 높더라도 오버피팅을 유발하고 이것을 다중공선성이라고 한다.\n",
    ">\n",
    "> 계수들의 합은 실제 계수와 동일하게 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f29758-e766-4ef3-ab17-9315426656a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = value)\n",
    "## df_train, df_test를 순서대로 산출한다. 랜덤으로 샘플을 추출하는 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efb5d8-a0fb-4b80-a5ae-5796145b88c8",
   "metadata": {},
   "source": [
    "### **10. Lasso, Ridge**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1198c-639b-4fab-ad58-aa67945b0a42",
   "metadata": {},
   "source": [
    "`Lasso` : L1 penalty, `LogisticRegressionCV`의 옵션 `penalty = 'l1' solver = 'liblinear'`로 사용가능\n",
    "\n",
    ": 다수의 coef 값들을 0으로 만드는 수학적 장치\n",
    "\n",
    "> 패널티 : 상관성이 짙은 설명변수들의 계수값의 절대값을 구한 뒤에 합치고(L1-norm을 구하고), 그 값이 0에서 떨어져 있을수록 패널티 부여.\n",
    ">\n",
    "> **불필요한 설명변수는 오버피팅을 유발하니 몇 개만 남기고 배제함**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`Ridge` : L2 penalty,  `LogisticRegressionCV`의 옵션 `penalty = 'l2'`로 사용가능\n",
    "\n",
    ": coef의 값들을 가중치에 따라 분할하는 수학적 장치.\n",
    "\n",
    "> 패널티 : 상관성이 짙은 설명변수들의 계수값을 제곱한 뒤 합치고(L2-norm을 구하고), 그 값이 0에서 떨어져 있을수록 패널티 부여.\n",
    ">\n",
    "> **불필요한 설명변수는 오버피팅을 유발하니 불필요한 녀석이 없도록 만듦**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270d2f5-4599-412b-a5ee-55c469a7429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.linear_model.Lasso(alpha = float)\n",
    "sklearn.linear_model.LassoCV(alphas = list of floats)\n",
    "## alpha의 스케일이 작음. 0~2 정도\n",
    "\n",
    "sklearn.linear_model.Ridge(alpha = float)\n",
    "sklearn.linear_model.RidgeCV(alphas = list of floats)\n",
    "## alpha의 스케일이 큼. 5e2 ~ 5e10 정도"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
